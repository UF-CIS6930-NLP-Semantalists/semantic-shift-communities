{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!pip install openpyxl\n",
    "!pip install transformers\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3e2378-f175-43fc-872c-0a333d0e0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "import string\n",
    "\n",
    "from typing import List, Set\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b74aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(data) -> str:\n",
    "    return data if isinstance(data, str) else str(data)\n",
    "\n",
    "def tokenize(text: str) -> List[Token]:\n",
    "  doc = nlp(text)\n",
    "  return [w for sent in doc.sents for w in sent]\n",
    "\n",
    "def remove_punctuation(tokens: List[Token]) -> List[Token]:\n",
    "  return [t for t in tokens if t.text not in string.punctuation]\n",
    "\n",
    "def remove_stop_words(tokens: List[Token]) -> List[Token]:\n",
    "  return [t for t in tokens if not t.is_stop]\n",
    "\n",
    "def lemmatize(tokens: List[Token]) -> List[str]:\n",
    "  return [t.lemma_ for t in tokens]\n",
    "\n",
    "def case_fold(tokens: List[str]) -> List[str]:\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def pre_process_text(text: str) -> List[str]:\n",
    "    return case_fold(lemmatize(remove_punctuation(tokenize(convert_to_string(text)))))\n",
    "\n",
    "def get_num_sentences(data) -> int:\n",
    "    doc = nlp(data if isinstance(data, str) else str(data))\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "def get_num_words(data) -> int:\n",
    "    doc = nlp(data if isinstance(data, str) else str(data))\n",
    "    sentence_length = 0\n",
    "    for sent in doc.sents: sentence_length += len(sent)\n",
    "    return sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ee110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conservative_df = pd.read_csv('cons_comments.csv', header=0)\n",
    "# liberal_df = pd.concat([pd.read_excel('lib_comments.xlsx', header=0), pd.read_excel('dem_comments.xlsx', header=0)])\n",
    "conservative_df = pd.read_csv('conservative_metadata_and_preprocessed_with_stop_words.csv', header=0)\n",
    "liberal_df = pd.read_csv('liberal_metadata_preprocessed_with_stop_words.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a510b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conservative_df['preprocess_body'] = conservative_df['body'].apply(pre_process_text)\n",
    "conservative_df['num_sentences'] = conservative_df['body'].apply(get_num_sentences)\n",
    "conservative_df['numWords'] = conservative_df['body'].apply(get_num_words)\n",
    "conservative_df.to_csv('conservative_metadata_and_preprocessed_with_stop_words.csv', index = False)\n",
    "\n",
    "liberal_df['preprocess_body'] = liberal_df['body'].apply(pre_process_text)\n",
    "liberal_df['num_sentences'] = liberal_df['body'].apply(get_num_sentences)\n",
    "liberal_df['numWords'] = liberal_df['body'].apply(get_num_words)\n",
    "liberal_df.to_csv('liberal_metadata_preprocessed_with_stop_words.csv', index = False)\n",
    "# random_sample_size = 194944\n",
    "# conservative_base_sample_df = conservative_df.sample(n=194944)\n",
    "# conservative_base_sample_df.to_csv('conservative_base_sample.csv', index = False)\n",
    "# liberal_base_sample_df = liberal_df\n",
    "# conservative_2k_sample = conservative_base_sample_df.sample(n=2000)\n",
    "# liberal_2k_sample = liberal_base_sample_df.sample(n=2000)\n",
    "# conservative_2k_sample.to_csv('conservative_2k_sample.csv', index = False)\n",
    "# liberal_2k_sample.to_csv('liberal_2k_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data: List[List[str]]) -> Set[str]:\n",
    "    data = list(map(lambda x: x.strip('[]').split(\",\") if type(x) == str else [], data))\n",
    "    vocab = {str(token).lower() for tokens in data for token in tokens}\n",
    "    return vocab\n",
    "\n",
    "def find_common_vocab(vocab1, vocab2):\n",
    "    return list(vocab1.intersection(vocab2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative_vocab = create_vocab(conservative_df['preprocess_body'])\n",
    "liberal_vocab = create_vocab(liberal_df['preprocess_body'])\n",
    "\n",
    "common_vocab = find_common_vocab(conservative_vocab, liberal_vocab)\n",
    "conservative_vocab_percent = len(conservative_vocab) / len(common_vocab) * 100\n",
    "liberal_vocab_percent = len(liberal_vocab) / len(common_vocab) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835a07a2-c5e2-4cdd-9a96-5a3cc542ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_meta_data(dataframe):\n",
    "    word_to_doc_map = collections.defaultdict(set)\n",
    "    word_to_score_map = collections.defaultdict(int)\n",
    "    word_to_freq_map = collections.defaultdict(int)\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        doc_id, score = row[\"id\"], row[\"score\"]\n",
    "        try:\n",
    "            for word in row[\"preprocess_body\"].strip('[]').split(','):\n",
    "                word_to_doc_map[word].add(doc_id)\n",
    "                word_to_score_map[word] += score\n",
    "                word_to_freq_map[word] += 1\n",
    "        except Exception as e:\n",
    "            print(\"Row: \", row[\"preprocess_body\"])\n",
    "            print(\"The row might be a non string value: \", e)\n",
    "    \n",
    "    meta_data = {\"word\": [], \"freq\": [], \"score\": [], \"doc_ids\": []}\n",
    "    for word, freq in word_to_freq_map.items():\n",
    "        meta_data[\"word\"].append(word)\n",
    "        meta_data[\"freq\"].append(freq)\n",
    "        meta_data[\"score\"].append(word_to_score_map.get(word, 0))\n",
    "        meta_data[\"doc_ids\"].append(word_to_doc_map.get(word, []))\n",
    "        \n",
    "    return pd.DataFrame(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abe507-015a-43c3-85b5-2d2c42105b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = ['conservative', 'liberal']\n",
    "colors = ['red', 'blue']\n",
    "size = [len(conservative_df['body']), len(liberal_df['body'])]\n",
    "vocab_percent = [conservative_vocab_percent , liberal_vocab_percent]\n",
    "vocab_size = [len(conservative_vocab), len(liberal_vocab)]\n",
    "average_sent_length = [np.mean(conservative_df['num_sentences']),np.mean(liberal_df['num_sentences'])]\n",
    "average_word_count = [np.mean(conservative_df['numWords']), np.mean(liberal_df['numWords'])]\n",
    "\n",
    "# post size\n",
    "plt.bar(x, size, color=colors)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Reddit Posts size comparison')\n",
    "for i, v in enumerate(size):\n",
    "    plt.text(i, v + 1, str(v), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "\n",
    "# vocabulary distribution\n",
    "chart = plt.pie(vocab_percent, labels=x, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Vocabulary Distribution')\n",
    "legend_labels = []\n",
    "for i in range(len(vocab_size)):\n",
    "    legend_labels.append(f\"{x[i]}: {vocab_size[i]}\")\n",
    "plt.legend(chart[0], legend_labels, title='Category', loc='center left')\n",
    "plt.show()\n",
    "\n",
    "# Sentence length\n",
    "plt.barh(x, average_sent_length, color=colors)\n",
    "plt.xlabel('Number of sentences per post')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Average Sentences per Post')\n",
    "for i, v in enumerate(average_sent_length):\n",
    "    plt.text(v + 0.3, i, str(round(v, 2)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "\n",
    "# Number of words\n",
    "plt.scatter(x, average_word_count, s=[a * 25 for a in average_word_count], alpha=0.5, c=colors, cmap='viridis')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of words per post')\n",
    "plt.title('Average word count')\n",
    "for i, v in enumerate(average_word_count):\n",
    "    plt.text(i, v, str(int(v)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c0fae-76ba-4a56-a4c5-bce1495fd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative_words = get_meta_data(conservative_df).sort_values('score', ascending = False)[\"word\"].head(100)\n",
    "liberal_words = get_meta_data(liberal_df).sort_values('score', ascending = False)[\"word\"].head(100)\n",
    "intersection = set(conservative_words).intersection(set(liberal_words))\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d945a3c8-80b2-4e0f-8c2e-4d54f4d9f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_interest = ['conservative']\n",
    "# ,'trump','democrat','election','party','job','news','vote','president','american','right','pay','run','government','country','democrats','bernie','biden','state','life','support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5c83981-7901-4575-87d9-30c480004676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row:  nan\n",
      "The row might be a non string value:  'float' object has no attribute 'strip'\n",
      "Row:  nan\n",
      "The row might be a non string value:  'float' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "similar_sentences_group = {}\n",
    "\n",
    "conservative_metadata = get_meta_data(conservative_df)\n",
    "liberal_metadata = get_meta_data(liberal_df)\n",
    "def padWord(word): return \"'\" + str(word) + \"'\"\n",
    "\n",
    "\n",
    "for word in list(map(lambda x: padWord(x), words_of_interest)):\n",
    "    similar_sentences_group[word] = defaultdict(list)\n",
    "    conservative_docs = list(set([postId \n",
    "                                  for postSet in conservative_metadata[conservative_metadata[\"word\"] == word][\"doc_ids\"]\n",
    "                                  for postId in postSet]\n",
    "                                )\n",
    "                            )\n",
    "    conservative_docs = list(map(lambda x: x.strip(\"'\"), conservative_docs))\n",
    "    for index, row in conservative_df[conservative_df[\"id\"].isin(conservative_docs)].iterrows():\n",
    "        doc = nlp(row[\"body\"])\n",
    "        similar_sentences_group[word][\"conservative\"].extend([sent for sent in doc.sents])\n",
    "     \n",
    "    \n",
    "    liberal_docs = list(set([postId \n",
    "                             for postSet in liberal_metadata[liberal_metadata[\"word\"] == word][\"doc_ids\"]\n",
    "                             for postId in postSet]\n",
    "                           )\n",
    "                       )\n",
    "    liberal_docs = list(map(lambda x: x.strip(\"'\"), liberal_docs))\n",
    "    for index, row in liberal_df[liberal_df[\"id\"].isin(liberal_docs)].iterrows():\n",
    "        doc = nlp(row[\"body\"])\n",
    "        similar_sentences_group[word][\"liberal\"].extend([sent for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4297cb03-5e79-4dd4-8a60-f78356541e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\n",
      "etag tracker:  \"fb140275c155a9c7c5a3b3e0e77a9e839594a938\"\n",
      "45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json\n",
      "etag tracker:  \"a661b1a138dac6dc5590367402d100765010ffd6\"\n",
      "c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json\n",
      "etag tracker:  \"949a6f013d67eb8a5b4b5b46026217b888021b88\"\n",
      "534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "etag tracker:  \"45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d\"\n",
      "3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "etag tracker:  \"45a2321a7ecfdaaf60a6c1fd7f5463994cc8907d\"\n",
      "3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e\n",
      "https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin\n",
      "etag tracker:  \"097417381d6c7230bd9e3557456d726de6e83245ec8b24f529f60198a67b203a\"\n",
      "a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
    "model.eval()\n",
    "\n",
    "def preprocess_cosine_similarity(sent) -> np.array:\n",
    "    marked_text = \"[CLS] \" + sent.text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "        hidden_states = outputs[2]\n",
    "        token_vecs = hidden_states[-2][0]\n",
    "        sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "        return sentence_embedding\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity(vector_1, vector_2):\n",
    "    return F.cosine_similarity(vector_1, vector_2, dim=0).item()\n",
    "\n",
    "def find_most_similar_document_cosine(sentence_df, data_point) -> float:\n",
    "  sentence_df[\"similarity_score\"] = sentence_df[\"sentence_processed\"].apply(lambda x: cosine_similarity(x, data_point))\n",
    "  return sentence_df.iloc[sentence_df[\"similarity_score\"].idxmax()].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d67d28-1e95-4a42-898e-b89976dc2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for word in similar_sentences_group:\n",
    "    conservative_bucket_df = pd.DataFrame({\"sentence\": similar_sentences_group[word][\"conservative\"]})\n",
    "    conservative_bucket_df[\"sentence_processed\"] = conservative_bucket_df[\"sentence\"].apply(preprocess_cosine_similarity)\n",
    "    conservative_final_df = pd.DataFrame(columns=[\"sentence\", \"similar_sentence\"])\n",
    "    for index, row in conservative_bucket_df.iterrows():\n",
    "        conservative_final_df = pd.concat([conservative_final_df,\n",
    "                              pd.DataFrame(\n",
    "                                  [(row[\"sentence\"], \n",
    "                                   find_most_similar_document_cosine(conservative_bucket_df[conservative_bucket_df.index != index].reset_index(), row[\"sentence_processed\"]))\n",
    "                                  ], columns=[\"sentence\", \"similar_sentence\"]\n",
    "                                )\n",
    "                              ]\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    liberal_bucket_df = pd.DataFrame({\"sentence\": similar_sentences_group[word][\"liberal\"]})\n",
    "    liberal_bucket_df[\"sentence_processed\"] = liberal_bucket_df[\"sentence\"].apply(preprocess_cosine_similarity)\n",
    "    liberal_final_df = pd.DataFrame(columns=[\"sentence\", \"similar_sentence\"])\n",
    "    for index, row in liberal_bucket_df.iterrows():\n",
    "        liberal_final_df = pd.concat([liberal_final_df,\n",
    "                              pd.DataFrame(\n",
    "                                  [(row[\"sentence\"], \n",
    "                                   find_most_similar_document_cosine(liberal_bucket_df[liberal_bucket_df.index != index].reset_index(), row[\"sentence_processed\"]))\n",
    "                                  ], columns=[\"sentence\", \"similar_sentence\"]\n",
    "                                )\n",
    "                              ]\n",
    "                            )\n",
    "    \n",
    "conservative_final_df.to_csv(\"similar_sents_conservative.csv\",index = False)\n",
    "liberal_final_df.to_csv(\"similar_sents_liberal.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acf8d11-6c9e-42da-ad5f-7060101ff61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if torch.cuda.is_available():    \n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "#     print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# else:\n",
    "#     print('No GPU available, using the CPU instead.')\n",
    "#     device = torch.device(\"cpu\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1679199168956,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "nlp-1.2",
   "language": "python",
   "name": "nlp-1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
