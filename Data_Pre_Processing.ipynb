{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e69f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md\n",
    "!pip install openpyxl\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e2378-f175-43fc-872c-0a333d0e0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "import string\n",
    "\n",
    "from typing import List, Set\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b74aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_string(data) -> str:\n",
    "    return data if isinstance(data, str) else str(data)\n",
    "\n",
    "def tokenize(text: str) -> List[Token]:\n",
    "  doc = nlp(text)\n",
    "  return [w for sent in doc.sents for w in sent]\n",
    "\n",
    "def remove_punctuation(tokens: List[Token]) -> List[Token]:\n",
    "  return [t for t in tokens if t.text not in string.punctuation]\n",
    "\n",
    "def remove_stop_words(tokens: List[Token]) -> List[Token]:\n",
    "  return [t for t in tokens if not t.is_stop]\n",
    "\n",
    "def lemmatize(tokens: List[Token]) -> List[str]:\n",
    "  return [t.lemma_ for t in tokens]\n",
    "\n",
    "def case_fold(tokens: List[str]) -> List[str]:\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def pre_process_text(text: str) -> List[str]:\n",
    "    return case_fold(lemmatize(remove_punctuation(tokenize(convert_to_string(text)))))\n",
    "\n",
    "def get_num_sentences(data) -> int:\n",
    "    doc = nlp(data if isinstance(data, str) else str(data))\n",
    "    return len(list(doc.sents))\n",
    "\n",
    "def get_num_words(data) -> int:\n",
    "    doc = nlp(data if isinstance(data, str) else str(data))\n",
    "    sentence_length = 0\n",
    "    for sent in doc.sents: sentence_length += len(sent)\n",
    "    return sentence_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conservative_df = pd.read_csv('cons_comments.csv', header=0)\n",
    "# liberal_df = pd.concat([pd.read_excel('lib_comments.xlsx', header=0), pd.read_excel('dem_comments.xlsx', header=0)])\n",
    "conservative_df = pd.read_csv('conservative_metadata_and_preprocessed_with_stop_words.csv', header=0)\n",
    "liberal_df = pd.read_csv('liberal_metadata_preprocessed_with_stop_words.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a510b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conservative_df['preprocess_body'] = conservative_df['body'].apply(pre_process_text)\n",
    "conservative_df['num_sentences'] = conservative_df['body'].apply(get_num_sentences)\n",
    "conservative_df['numWords'] = conservative_df['body'].apply(get_num_words)\n",
    "conservative_df.to_csv('conservative_metadata_and_preprocessed_with_stop_words.csv', index = False)\n",
    "\n",
    "liberal_df['preprocess_body'] = liberal_df['body'].apply(pre_process_text)\n",
    "liberal_df['num_sentences'] = liberal_df['body'].apply(get_num_sentences)\n",
    "liberal_df['numWords'] = liberal_df['body'].apply(get_num_words)\n",
    "liberal_df.to_csv('liberal_metadata_preprocessed_with_stop_words.csv', index = False)\n",
    "# random_sample_size = 194944\n",
    "# conservative_base_sample_df = conservative_df.sample(n=194944)\n",
    "# conservative_base_sample_df.to_csv('conservative_base_sample.csv', index = False)\n",
    "# liberal_base_sample_df = liberal_df\n",
    "# conservative_2k_sample = conservative_base_sample_df.sample(n=2000)\n",
    "# liberal_2k_sample = liberal_base_sample_df.sample(n=2000)\n",
    "# conservative_2k_sample.to_csv('conservative_2k_sample.csv', index = False)\n",
    "# liberal_2k_sample.to_csv('liberal_2k_sample.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data: List[List[str]]) -> Set[str]:\n",
    "    data = list(map(lambda x: x.strip('[]').split(\",\") if type(x) == str else [], data))\n",
    "    vocab = {str(token).lower() for tokens in data for token in tokens}\n",
    "    return vocab\n",
    "\n",
    "def find_common_vocab(vocab1, vocab2):\n",
    "    return list(vocab1.intersection(vocab2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbb0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative_vocab = create_vocab(conservative_df['preprocess_body'])\n",
    "liberal_vocab = create_vocab(liberal_df['preprocess_body'])\n",
    "\n",
    "common_vocab = find_common_vocab(conservative_vocab, liberal_vocab)\n",
    "conservative_vocab_percent = len(conservative_vocab) / len(common_vocab) * 100\n",
    "liberal_vocab_percent = len(liberal_vocab) / len(common_vocab) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a07a2-c5e2-4cdd-9a96-5a3cc542ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_meta_data(dataframe):\n",
    "    word_to_doc_map = collections.defaultdict(set)\n",
    "    word_to_score_map = collections.defaultdict(int)\n",
    "    word_to_freq_map = collections.defaultdict(int)\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        doc_id, score = row[\"id\"], row[\"score\"]\n",
    "        try:\n",
    "            for word in row[\"preprocess_body\"].strip('[]').split(','):\n",
    "                word_to_doc_map[word].add(doc_id)\n",
    "                word_to_score_map[word] += score\n",
    "                word_to_freq_map[word] += 1\n",
    "        except Exception as e:\n",
    "            print(\"Row: \", row[\"preprocess_body\"])\n",
    "            print(\"The row might be a non string value: \", e)\n",
    "    \n",
    "    meta_data = {\"word\": [], \"freq\": [], \"score\": [], \"doc_ids\": []}\n",
    "    for word, freq in word_to_freq_map.items():\n",
    "        meta_data[\"word\"].append(word)\n",
    "        meta_data[\"freq\"].append(freq)\n",
    "        meta_data[\"score\"].append(word_to_score_map.get(word, 0))\n",
    "        meta_data[\"doc_ids\"].append(word_to_doc_map.get(word, []))\n",
    "        \n",
    "    return pd.DataFrame(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4abe507-015a-43c3-85b5-2d2c42105b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = ['conservative', 'liberal']\n",
    "colors = ['red', 'blue']\n",
    "size = [len(conservative_df['body']), len(liberal_df['body'])]\n",
    "vocab_percent = [conservative_vocab_percent , liberal_vocab_percent]\n",
    "vocab_size = [len(conservative_vocab), len(liberal_vocab)]\n",
    "average_sent_length = [np.mean(conservative_df['num_sentences']),np.mean(liberal_df['num_sentences'])]\n",
    "average_word_count = [np.mean(conservative_df['numWords']), np.mean(liberal_df['numWords'])]\n",
    "\n",
    "# post size\n",
    "plt.bar(x, size, color=colors)\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Reddit Posts size comparison')\n",
    "for i, v in enumerate(size):\n",
    "    plt.text(i, v + 1, str(v), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "\n",
    "# vocabulary distribution\n",
    "chart = plt.pie(vocab_percent, labels=x, autopct='%1.1f%%', colors=colors)\n",
    "plt.title('Vocabulary Distribution')\n",
    "legend_labels = []\n",
    "for i in range(len(vocab_size)):\n",
    "    legend_labels.append(f\"{x[i]}: {vocab_size[i]}\")\n",
    "plt.legend(chart[0], legend_labels, title='Category', loc='center left')\n",
    "plt.show()\n",
    "\n",
    "# Sentence length\n",
    "plt.barh(x, average_sent_length, color=colors)\n",
    "plt.xlabel('Number of sentences per post')\n",
    "plt.ylabel('Category')\n",
    "plt.title('Average Sentences per Post')\n",
    "for i, v in enumerate(average_sent_length):\n",
    "    plt.text(v + 0.3, i, str(round(v, 2)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n",
    "\n",
    "# Number of words\n",
    "plt.scatter(x, average_word_count, s=[a * 25 for a in average_word_count], alpha=0.5, c=colors, cmap='viridis')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of words per post')\n",
    "plt.title('Average word count')\n",
    "for i, v in enumerate(average_word_count):\n",
    "    plt.text(i, v, str(int(v)), color='black', fontweight='bold', ha='center')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352c0fae-76ba-4a56-a4c5-bce1495fd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative_words = get_meta_data(conservative_df).sort_values('score', ascending = False)[\"word\"].head(100)\n",
    "liberal_words = get_meta_data(liberal_df).sort_values('score', ascending = False)[\"word\"].head(100)\n",
    "intersection = set(conservative_words).intersection(set(liberal_words))\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d945a3c8-80b2-4e0f-8c2e-4d54f4d9f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_interest = ['conservative','trump','democrat','election','party','job','news','vote','president','american','right','pay','run','government','country','democrats','bernie','biden','state','life','support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c83981-7901-4575-87d9-30c480004676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "similar_sentences_group = {}\n",
    "\n",
    "conservative_metadata = get_meta_data(conservative_df)\n",
    "liberal_metadata = get_meta_data(liberal_df)\n",
    "def padWord(word): return \"'\" + str(word) + \"'\"\n",
    "\n",
    "\n",
    "for word in list(map(lambda x: padWord(x), words_of_interest)):\n",
    "    similar_sentences_group[word] = defaultdict(list)\n",
    "    conservative_docs = list(set([postId \n",
    "                                  for postSet in conservative_metadata[conservative_metadata[\"word\"] == word][\"doc_ids\"]\n",
    "                                  for postId in postSet]\n",
    "                                )\n",
    "                            )\n",
    "    conservative_docs = list(map(lambda x: x.strip(\"'\"), conservative_docs))\n",
    "    for index, row in conservative_df[conservative_df[\"id\"].isin(conservative_docs)].iterrows():\n",
    "        doc = nlp(row[\"body\"])\n",
    "        similar_sentences_group[word][\"conservative\"].extend([sent for sent in doc.sents])\n",
    "     \n",
    "    \n",
    "    liberal_docs = list(set([postId \n",
    "                             for postSet in liberal_metadata[liberal_metadata[\"word\"] == word][\"doc_ids\"]\n",
    "                             for postId in postSet]\n",
    "                           )\n",
    "                       )\n",
    "    liberal_docs = list(map(lambda x: x.strip(\"'\"), liberal_docs))\n",
    "    for index, row in liberal_df[liberal_df[\"id\"].isin(liberal_docs)].iterrows():\n",
    "        doc = nlp(row[\"body\"])\n",
    "        similar_sentences_group[word][\"liberal\"].extend([sent for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297cb03-5e79-4dd4-8a60-f78356541e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def preprocess_cosine_similarity(text: str) -> np.array:\n",
    "  tokens = [token for token in text]\n",
    "  tokens = remove_punctuation(tokens)\n",
    "  tokens = remove_stop_words(tokens)\n",
    "  vector = np.mean([word.vector for word in tokens], axis=0)\n",
    "  return vector\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(vector_1: np.array, vector_2: np.array) -> float:\n",
    "    cosine_distance = cosine(vector_1, vector_2, w=None) \n",
    "    return 1 - cosine_distance\n",
    "\n",
    "def find_most_similar_document_cosine(sentence_df, data_point: np.array) -> int:\n",
    "  sentence_df[\"similarity_score\"] = sentence_df[\"sentence_processed\"].apply(lambda x: cosine_similarity(x, data_point))\n",
    "  return sentence_df.iloc[sentence_df[\"similarity_score\"].idxmax()].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d67d28-1e95-4a42-898e-b89976dc2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in similar_sentences_group:\n",
    "    conservative_bucket_df = pd.DataFrame({\"sentence\": similar_sentences_group[word][\"conservative\"]})\n",
    "    conservative_bucket_df[\"sentence_processed\"] = conservative_bucket_df[\"sentence\"].apply(preprocess_cosine_similarity)\n",
    "    conservative_final_df = pd.DataFrame(columns=[\"sentence\", \"similar_sentence\"])\n",
    "    for index, row in conservative_bucket_df.iterrows():\n",
    "        conservative_final_df = pd.concat([conservative_final_df,\n",
    "                              pd.DataFrame(\n",
    "                                  [(row[\"sentence\"], \n",
    "                                   find_most_similar_document_cosine(conservative_bucket_df[conservative_bucket_df.index != index], row[\"sentence_processed\"]))\n",
    "                                  ], columns=[\"sentence\", \"similar_sentence\"]\n",
    "                                )\n",
    "                              ]\n",
    "                            )\n",
    "    \n",
    "    \n",
    "    liberal_bucket_df = pd.DataFrame({\"sentence\": similar_sentences_group[word][\"liberal\"]})\n",
    "    liberal_bucket_df[\"sentence_processed\"] = liberal_bucket_df[\"sentence\"].apply(preprocess_cosine_similarity)\n",
    "    liberal_final_df = pd.DataFrame(columns=[\"sentence\", \"similar_sentence\"])\n",
    "    for index, row in liberal_bucket_df.iterrows():\n",
    "        liberal_final_df = pd.concat([liberal_final_df,\n",
    "                              pd.DataFrame(\n",
    "                                  [(row[\"sentence\"], \n",
    "                                   find_most_similar_document_cosine(liberal_bucket_df[liberal_bucket_df.index != index], row[\"sentence_processed\"]))\n",
    "                                  ], columns=[\"sentence\", \"similar_sentence\"]\n",
    "                                )\n",
    "                              ]\n",
    "                            )\n",
    "    \n",
    "print(conservative_final_df.tail(5))\n",
    "print(liberal_final_df.tail(5))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1679199168956,
   "trusted": true
  },
  "kernelspec": {
   "display_name": "nlp-1.2",
   "language": "python",
   "name": "nlp-1.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
